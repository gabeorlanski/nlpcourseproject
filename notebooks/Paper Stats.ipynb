{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter,defaultdict\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 150)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# %matplotlib inline\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "data_path = PROJECT_ROOT.joinpath('data')\n",
    "print(PROJECT_ROOT)\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea06409",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name_map = {\n",
    "    'anli[dev_r1]': 'ANLI R1',\n",
    "    'anli[dev_r2]':  'ANLI R2',\n",
    "    'anli[dev_r3]':  'ANLI R3',\n",
    "    'AQuA[validation]':  'AQuA',\n",
    "    'CommitmentBank[validation]': 'CB',\n",
    "    'craigslist_bargains[validation]':'CraigslistBargains',\n",
    "    'RecognizingTextualEntailment[validation]':'RTE',\n",
    "    'WordsinContext[validation]': 'WiC',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b61dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = pd.read_csv(data_path.joinpath('baselines.csv'))\n",
    "baselines = baselines[baselines['group'].isin(group_name_map) & baselines['run_name'].isin(\n",
    "    ['T0','T5']\n",
    ")].copy()\n",
    "baselines['group'] = baselines['group'].apply(lambda x: group_name_map[x])\n",
    "\n",
    "all_cross_task = pd.read_csv(data_path.joinpath('cross_task.csv'))\n",
    "cross_task = all_cross_task[all_cross_task['run_name']=='CTBase'].copy()\n",
    "cross_task_prompts = cross_task.groupby(['name','prompt_id']).median().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_df = cross_task.groupby(['group','prompt_task']).rank()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prompts_df = cross_task[cross_task['training_task']].copy()\n",
    "unseen_prompts_df=  cross_task[(~cross_task['training_task']) & ~(cross_task['prompt_task']==\"No Prompt\")].copy()\n",
    "with_choices = cross_task[cross_task['choices_in_prompt']].copy()\n",
    "no_choices =  cross_task[~cross_task['choices_in_prompt']].copy()\n",
    "is_mcq = with_choices[with_choices['is_mcq']].copy()\n",
    "not_mcq =  with_choices[~with_choices['is_mcq']].copy()\n",
    "\n",
    "diff_added_text =  pd.read_csv(data_path.joinpath('diff.csv'))\n",
    "diff_added_text = diff_added_text[diff_added_text['run_name'].isin([\"CTBase\",\"CTNoText\"])]\n",
    "no_text = diff_added_text[diff_added_text['run_name']=='CTNoText']\n",
    "with_text = diff_added_text[diff_added_text['run_name']=='CTBase']\n",
    "\n",
    "\n",
    "def get_ablation_stats(name, df):\n",
    "    if \"Extra Text\" not in name:\n",
    "        f1_stats = df.groupby(['name','prompt_id']).median().describe()['f1_rank']\n",
    "        acc_stats = df.groupby(['name','prompt_id']).median().describe()['accuracy_rank']\n",
    "    else:\n",
    "        f1_stats = df.groupby(['name','prompt_id']).median().describe()['f1_rank']\n",
    "        acc_stats = df.groupby(['name','prompt_id']).median().describe()['accuracy_rank']\n",
    "    return ({\n",
    "        'name':name,\n",
    "#         \"Accuracy Count\": acc_stats['count'],\n",
    "        'Accuracy Mean':acc_stats['mean'],\n",
    "        'Accuracy Median':acc_stats['50%'],\n",
    "        'Accuracy Q1':acc_stats['25%'],\n",
    "        'Accuracy Q3':acc_stats['75%'],\n",
    "        'F1 Mean':f1_stats['mean'],\n",
    "        'F1 Median':f1_stats['50%'],\n",
    "        'F1 Q1':f1_stats['25%'],\n",
    "        'F1 Q3':f1_stats['75%']\n",
    "    })\n",
    "\n",
    "\n",
    "all_stats = []\n",
    "# all = []\n",
    "ablations = {\n",
    "    \"Training Prompts\":training_prompts_df,\n",
    "    \"Unseen Prompts\":unseen_prompts_df,\n",
    "    \"With Choices\":with_choices,\n",
    "    \"No Choices\": no_choices,\n",
    "    \"Is MCQ\":is_mcq,\n",
    "    \"Not MCQ\":not_mcq,\n",
    "    \"Extra Text\":with_text,\n",
    "    \"No Extra Text\":no_text,\n",
    "}\n",
    "for a,b in ablations.items():\n",
    "    all_stats.append(get_ablation_stats(a,b))\n",
    "all_stats = pd.DataFrame.from_records(all_stats,index=[\"name\"])\n",
    "all_stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42269fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = cross_task[cross_task['run_name']==\"CTBase\"].corr()\n",
    "# corr_df = cross_task_prompts.corr()\n",
    "corr_df = corr_df.loc[\n",
    "    [\"choices_in_prompt\",\"is_mcq\",\"training_task\",\"prompt_tokens\"],\n",
    "    [\"accuracy_rank\",\"f1_rank\"]\n",
    "]\n",
    "# text_vs_no_text = pd.read_csv(data_path.joinpath('diff.csv'))\n",
    "# text_vs_no_text = text_vs_no_text[text_vs_no_text['run_name'].isin(['CTBase','CTNoText'])]\n",
    "# text_vs_no_text['extra_text'] = text_vs_no_text['run_name'].apply(lambda l: 1 if l==\"CTBase\" else 0)\n",
    "\n",
    "# corr_df = corr_df.append(text_vs_no_text.corr().loc[['extra_text'],['accuracy_rank','f1_rank']])\n",
    "print(corr_df.to_latex(float_format='{:0.2f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e3960",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vs_no_text[['run_name','extra_text','accuracy_rank']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9618c4a",
   "metadata": {},
   "source": [
    "# Training Prompts Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5991cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prompts_df = cross_task[cross_task['training_task']].copy()\n",
    "unseen_prompts_df=  cross_task[(~cross_task['training_task']) & ~(cross_task['prompt_task']==\"No Prompt\")].copy()\n",
    "\n",
    "print(f\"Training Prompts:\")\n",
    "print(f\"\\tF1 Rank:{training_prompts_df['f1_rank'].median():0.2f}\")\n",
    "print(f\"\\tAccuracy Rank:{training_prompts_df['accuracy_rank'].median():0.2f}\")\n",
    "\n",
    "print(f\"Unseen Prompts:\")\n",
    "print(f\"\\tF1 Rank:{unseen_prompts_df['f1_rank'].median():0.2f}\")\n",
    "print(f\"\\tAccuracy Rank:{unseen_prompts_df['accuracy_rank'].median():0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce867c23",
   "metadata": {},
   "source": [
    "# Logit Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580379d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73138d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    1,\n",
    "    2,\n",
    "    subplot_titles=[\"Accuracy\",\"F1\"],\n",
    ")\n",
    "\n",
    "acc_chart = go.Scatter(\n",
    "    x=cross_task_prompts['logits/range_mean'].tolist(),\n",
    "    y=cross_task_prompts['accuracy_rank'].tolist(),\n",
    "    mode='markers',\n",
    "    showlegend=False\n",
    "    \n",
    ")\n",
    "\n",
    "f1_chart = go.Scatter(\n",
    "    x=cross_task_prompts['logits/range_mean'].tolist(),\n",
    "    y=cross_task_prompts['f1_rank'].tolist(),\n",
    "    mode='markers',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.append_trace(f1_chart, row=1, col=2)\n",
    "fig.append_trace(acc_chart, row=1, col=1)\n",
    "fig.update_layout(\n",
    "#                 title=title+\" - \"+met_name,\n",
    "    title_x=0.5,\n",
    "    font=dict(size=15),\n",
    "    template=\"plotly_white\",\n",
    "    legend_orientation='h',\n",
    "    legend=dict(xanchor=\"center\", x=0.5, bgcolor=\"rgba(0,0,0,0)\"),\n",
    "    # yaxis=dict(range=[0,100]),\n",
    "    width=1000,\n",
    "    height=600,\n",
    ")\n",
    "fig['layout']['xaxis']['title']='Mean Range of the Log Probabilities'\n",
    "fig['layout']['xaxis2']['title']='Mean Range of the Log Probabilities'\n",
    "fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
    "fig['layout']['yaxis2']['autorange'] = \"reversed\"\n",
    "fig['layout']['yaxis']['title']='Median Rank out of 98 Prompts'\n",
    "fig['layout']['yaxis2']['title']='Median Rank out of 98 Prompts'\n",
    "fig.update_yaxes(range=[0,100])# hide all the xticks\n",
    "fig.write_image(re.sub(' ', '-', 'shared_ranks_graphs.png'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ee9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = cross_task_prompts.corrwith(cross_task_prompts['logits/range_mean']).dropna()\n",
    "\n",
    "corr = corr.drop([c for c in corr.index.values if 'logits/' in c])\n",
    "\n",
    "corr.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3fbbd",
   "metadata": {},
   "source": [
    "# Text vs No Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_added_text =  pd.read_csv(data_path.joinpath('diff.csv'))\n",
    "diff_added_text = diff_added_text[diff_added_text['run_name'].isin([\"CTBase\",\"CTNoText\"])]\n",
    "diff_added_text = diff_added_text.groupby([\"run_name\",\"group\",'prompt_task','prompt_id']).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = (\n",
    "    diff_added_text.loc[\"CTBase\"] - diff_added_text.loc['CTNoText']\n",
    ")\n",
    "\n",
    "difference[\"accuracy\"] = difference['accuracy'] /diff_added_text.loc['CTNoText','accuracy']*100\n",
    "difference[\"f1\"] = difference['f1'] /diff_added_text.loc['CTNoText','f1']*100\n",
    "\n",
    "difference.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "difference= difference.drop([\"choices_in_prompt\",\"training_task\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdbcf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference.reset_index().groupby([\"group\",\"prompt_task\"]).describe().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aaebc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bc6abe3",
   "metadata": {},
   "source": [
    "# Choices in Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_choices = cross_task[cross_task['choices_in_prompt']].copy()\n",
    "no_choices =  cross_task[~cross_task['choices_in_prompt']].copy()\n",
    "\n",
    "print(f\"With Choices:\")\n",
    "print(f\"\\tF1 Rank:{with_choices['f1_rank'].median():0.2f}\")\n",
    "print(f\"\\tAccuracy Rank:{with_choices['accuracy_rank'].median():0.2f}\")\n",
    "\n",
    "print(f\"No Choices:\")\n",
    "print(f\"\\tF1 Rank:{no_choices['f1_rank'].median():0.2f}\")\n",
    "print(f\"\\tAccuracy Rank:{no_choices['accuracy_rank'].median():0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaec20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = cross_task_prompts.corrwith(cross_task_prompts['prompt_tokens']).dropna()\n",
    "corr = corr.drop([\"prompt_tokens\"])\n",
    "corr.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = cross_task_prompts.corrwith(cross_task_prompts['is_mcq']).dropna()\n",
    "corr = corr.drop([\"is_mcq\"])\n",
    "corr.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8a915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpcourseproject",
   "language": "python",
   "name": "nlpcourseproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
